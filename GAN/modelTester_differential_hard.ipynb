{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15424352845930569892\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9718310882\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8613941259193930723\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, LSTM, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape, ZeroPadding1D, Cropping1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_domain = None\n",
    "test_data_phase = None\n",
    "\n",
    "with open('dataset/test_samples.pickle', 'rb') as data:\n",
    "    test_samples = pickle.load(data)\n",
    "\n",
    "with open('generatedData/3/generatedData.pickle', 'rb') as data:\n",
    "    generated_data = pickle.load(data)\n",
    "\n",
    "with open('dataset/test_set_differential/test_set_fdi.pickle', 'rb') as data:\n",
    "    test_data_fdi = pickle.load(data)\n",
    "    \n",
    "with open('dataset/test_set_differential/test_set_tap_setting_hard.pickle', 'rb') as data:\n",
    "    test_data_tap_setting = pickle.load(data)\n",
    "    \n",
    "with open('dataset/test_set_differential/test_set_replay_hard.pickle', 'rb') as data:\n",
    "    test_data_replay = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data separation according to window size\n",
    "sequenceLen = 48 # in training\n",
    "testSequenceLen = test_samples.shape[1]\n",
    "testSamplesSize = test_samples.shape[0]\n",
    "dimensionsCount = 6\n",
    "\n",
    "testSetSize = testSamplesSize + generated_data.shape[0]\n",
    "numberOfAttackSamples = generated_data.shape[0]\n",
    "attackSamplesStartIndex = testSetSize - numberOfAttackSamples\n",
    "\n",
    "test_data = np.concatenate((test_samples, generated_data));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14148, 95, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = list()\n",
    "for entry in test_data:\n",
    "    for i in range(48):\n",
    "        testSet.append(list(entry[i:i+48,:]))\n",
    "testSet = np.array(testSet)\n",
    "ConvTestData = np.reshape(testSet, (testSetSize*48,sequenceLen*dimensionsCount,1), order='C')\n",
    "fullyConnectedTestData = np.reshape(testSet, (testSetSize*48,sequenceLen*dimensionsCount), order='C')\n",
    "LSTMTestData = np.reshape(testSet, (testSetSize*48,sequenceLen,dimensionsCount), order='C')\n",
    "    \n",
    "test_labels = np.zeros(testSetSize)\n",
    "test_labels[attackSamplesStartIndex:testSetSize] = np.ones(numberOfAttackSamples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Here we defined rankedPrecision() and rankedRecal() functions which are our evaluation functions in anomaly detection. The overall idea here is that, we first sort the samples loss from highest to lowest. Then, calculate precision and recall in top-K samples using definitions below:<br>\n",
    "<p style=\"text-align:center\">\n",
    "$Precision=\\frac{True Positive}{True Positive + False Positive}$<br/><br/>\n",
    "$Recall=\\frac{True Positive}{True Positive + False Negative}$<br/><br/>\n",
    "</p>\n",
    "As we know the number of anomalies in our test set, we pick K to be that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankedPrecision(mse_labels):\n",
    "    sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "    totalNumberOfAnomalies = sum(sorted_mse_label[:,1] == 1)\n",
    "    TP = sum(sorted_mse_label[0:totalNumberOfAnomalies,1] == 1)\n",
    "    FP = sum(sorted_mse_label[0:totalNumberOfAnomalies,1] == 0)\n",
    "    TN = sum(sorted_mse_label[totalNumberOfAnomalies:,1] == 0)\n",
    "    FN = sum(sorted_mse_label[totalNumberOfAnomalies:,1] == 1)\n",
    "    precision = TP/(TP+FP)\n",
    "    return precision\n",
    "    \n",
    "    \n",
    "def rankedRecall(mse_labels):\n",
    "    sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "    totalNumberOfAnomalies = sum(sorted_mse_label[:,1] == 1)\n",
    "    TP = sum(sorted_mse_label[0:totalNumberOfAnomalies,1] == 1)\n",
    "    FP = sum(sorted_mse_label[0:totalNumberOfAnomalies,1] == 0)\n",
    "    TN = sum(sorted_mse_label[totalNumberOfAnomalies:,1] == 0)\n",
    "    FN = sum(sorted_mse_label[totalNumberOfAnomalies:,1] == 1)\n",
    "    recall = TP/(TP+FN)\n",
    "    return recall\n",
    "\n",
    "# show_curve() is for plotting training and validation losses\n",
    "def show_curve(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankedPrecisionThre(mse_labels,threshold):\n",
    "    sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "    TP = sum((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] >= threshold))\n",
    "    FP = sum((sorted_mse_label[:,1] == 0) & (sorted_mse_label[:,0] >= threshold))\n",
    "    TN = sum((sorted_mse_label[:,1] == 0) & (sorted_mse_label[:,0] < threshold))\n",
    "    FN = sum((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] < threshold))\n",
    "    precision = TP/(TP+FP)\n",
    "    return precision\n",
    "    \n",
    "    \n",
    "def rankedRecallThre(mse_labels,threshold):\n",
    "    sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "    TP = sum((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] >= threshold))\n",
    "    FP = sum((sorted_mse_label[:,1] == 0) & (sorted_mse_label[:,0] >= threshold))\n",
    "    TN = sum((sorted_mse_label[:,1] == 0) & (sorted_mse_label[:,0] < threshold))\n",
    "    FN = sum((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] < threshold))\n",
    "    recall = TP/(TP+FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D Convolutional Network Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is:  1.0\n",
      "Recall is:  1.0\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "model = load_model('models_differential/conv.h5')\n",
    "\n",
    "predicted = model.predict(ConvTestData)\n",
    "mse = (np.square(ConvTestData - predicted)).mean(axis=1)\n",
    "mse = mse.reshape(testSetSize,48)\n",
    "mse = mse.mean(axis=1)\n",
    "mse_label = np.vstack((mse, test_labels)).T\n",
    "precision = rankedPrecision(mse_label)\n",
    "recall = rankedRecall(mse_label)\n",
    "precisions.append(precision)\n",
    "recalls.append(recall)\n",
    "print(\"Precision is: \", precision)\n",
    "print(\"Recall is: \", recall)\n",
    "\n",
    "convNetPrecisions_domain = precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4760437932762619e-05\n",
      "3.0375225336844425e-06\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "\n",
    "totalNumberOfAnomalies = sum(sorted_mse_label[:,1] == 1)\n",
    "print(min(sorted_mse_label[sorted_mse_label[:,1] == 1,0])) # 0.010118610983828593\n",
    "print(max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "Precision by threshold is:  1.0\n",
      "Recall by threshold is:  1.0\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "minMSE = min(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "maxMSE = max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "thre = maxMSE + (maxMSE - minMSE)/10\n",
    "threshold = 1\n",
    "print((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] >= threshold))\n",
    "\n",
    "precision = rankedPrecisionThre(mse_label,thre)\n",
    "recall = rankedRecallThre(mse_label,thre)\n",
    "print(\"Precision by threshold is: \", precision)\n",
    "print(\"Recall by threshold is: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encoder Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is:  1.0\n",
      "Recall is:  1.0\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "model = load_model('models_differential/autoencoder.h5')\n",
    "\n",
    "predicted = model.predict(fullyConnectedTestData)\n",
    "mse = (np.square(fullyConnectedTestData - predicted)).mean(axis=1)\n",
    "mse = mse.reshape(testSetSize,48)\n",
    "mse = mse.mean(axis=1)\n",
    "mse_label = np.vstack((mse, test_labels)).T\n",
    "precision = rankedPrecision(mse_label)\n",
    "recall = rankedRecall(mse_label)\n",
    "precisions.append(precision)\n",
    "recalls.append(recall)\n",
    "print(\"Precision is: \", precision)\n",
    "print(\"Recall is: \", precision)\n",
    "\n",
    "fullyConnectedPrecisions_domain = precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001339250640816447\n",
      "0.00012368260877431675\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "\n",
    "totalNumberOfAnomalies = sum(sorted_mse_label[:,1] == 1)\n",
    "print(min(sorted_mse_label[sorted_mse_label[:,1] == 1,0])) # 0.010118610983828593\n",
    "print(max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "Precision by threshold is:  1.0\n",
      "Recall by threshold is:  1.0\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "minMSE = min(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "maxMSE = max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "thre = maxMSE + (maxMSE - minMSE)/10\n",
    "threshold = 1\n",
    "print((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] >= threshold))\n",
    "\n",
    "precision = rankedPrecisionThre(mse_label,thre)\n",
    "recall = rankedRecallThre(mse_label,thre)\n",
    "print(\"Precision by threshold is: \", precision)\n",
    "print(\"Recall by threshold is: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSTM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is:  1.0\n",
      "Recall is:  1.0\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "model = load_model('models_differential/lstm.h5')\n",
    "predicted = model.predict(LSTMTestData)\n",
    "mse = (np.square(LSTMTestData - predicted)).mean(axis=2).mean(axis=1)\n",
    "mse = mse.reshape(testSetSize,48)\n",
    "mse = mse.mean(axis=1)\n",
    "mse_label = np.vstack((mse, test_labels)).T\n",
    "precision = rankedPrecision(mse_label)\n",
    "recall = rankedRecall(mse_label)\n",
    "precisions.append(precision)\n",
    "recalls.append(recall)\n",
    "print(\"Precision is: \", precision)\n",
    "print(\"Recall is: \", precision)\n",
    "\n",
    "LSTMPrecisions_domain = precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.721195468540186e-06\n",
      "2.3945688877362906e-07\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "\n",
    "totalNumberOfAnomalies = sum(sorted_mse_label[:,1] == 1)\n",
    "print(min(sorted_mse_label[sorted_mse_label[:,1] == 1,0])) # 0.010118610983828593\n",
    "print(max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "Precision by threshold is:  1.0\n",
      "Recall by threshold is:  1.0\n"
     ]
    }
   ],
   "source": [
    "sorted_mse_label = mse_label[mse_label[:,0].argsort()[::-1]]\n",
    "minMSE = min(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "maxMSE = max(sorted_mse_label[sorted_mse_label[:,1] == 0,0])\n",
    "thre = maxMSE + (maxMSE - minMSE)/10\n",
    "threshold = 1\n",
    "print((sorted_mse_label[:,1] == 1) & (sorted_mse_label[:,0] >= threshold))\n",
    "\n",
    "precision = rankedPrecisionThre(mse_label,thre)\n",
    "recall = rankedRecallThre(mse_label,thre)\n",
    "print(\"Precision by threshold is: \", precision)\n",
    "print(\"Recall by threshold is: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
